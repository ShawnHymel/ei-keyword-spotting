{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ei-audio-dataset-curation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMkAgYc8MuCq"
      },
      "source": [
        "# Keyword Spotting Dataset Curation\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/ei-keyword-spotting/blob/master/ei-audio-dataset-curation.ipynb)\n",
        "\n",
        "Use this tool to download the Google Speech Commands Dataset, combine it with your own keywords, mix in some background noise, and upload the curated dataset to Edge Impulse. From there, you can train a neural network to classify spoken words and upload it to a microcontroller to perform real-time keyword spotting.\n",
        "\n",
        " 1. Upload samples of your own keyword (optional)\n",
        " 2. Adjust parameters in the Settings cell (you will need an [Edge Impulse](https://www.edgeimpulse.com/) account)\n",
        " 3. Run the rest of the cells! ('shift' + 'enter' on each cell)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAn75H1iPYOD"
      },
      "source": [
        "### Upload your own keyword samples\n",
        "You are welcome to use my [custom keyword dataset](https://github.com/ShawnHymel/custom-speech-commands-dataset), but note that it's limited and that I can't promise it will work well. If you want to use it, uncomment the `###Download custom dataset` cell below. You may also add your own recorded keywords to the extracted folder (`/content/custom_keywords`) to augment what's already there.\n",
        "\n",
        "If you'd rather upload your own custom keyword dataset, follow these instructions:\n",
        "\n",
        "On the left pane, in the file browser, create a directory structure containing space for your keyword audio samples. All samples for each keyword should be in a directory with that keyword's name. \n",
        "\n",
        "The audio samples should be `.wav` format, mono, and 1 second long. Bitrate and bitdepth should not matter. Samples shorter than 1 second will be padded with 0s, and samples longer than 1 second will be truncated to 1 second. The exact name of each `.wav` file does not matter, as they will be read, mixed with background noise, and saved to a separate file with an auto-generated name. Directory name does matter (it is used to determine the name of the class during neural network training).\n",
        "\n",
        "Right-click on each keyword directory and upload all of your samples. Your directory structor should look like the following:\n",
        "\n",
        "```\n",
        "/\n",
        "|- content\n",
        "|--- custom_keywords\n",
        "|----- keyword_1\n",
        "|------- 000.wav\n",
        "|------- 001.wav\n",
        "|------- ...\n",
        "|----- keyword_2\n",
        "|------- 000.wav\n",
        "|------- 001.wav\n",
        "|------- ...\n",
        "|----- ...\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81cDNtYQj-ao"
      },
      "source": [
        "### Update Node.js to the latest stable version\n",
        "!npm cache clean -f\n",
        "!npm install -g n\n",
        "!n 16.18.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjn7Y0iPXCh"
      },
      "source": [
        "### Install required packages and tools\n",
        "!python -m pip install soundfile\n",
        "!npm install -g --unsafe-perm edge-impulse-cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06YDTU0c0-H"
      },
      "source": [
        "### Settings (You probably do not need to change these)\n",
        "BASE_DIR = \"/content\"\n",
        "OUT_DIR = \"keywords_curated\"\n",
        "GOOGLE_DATASET_FILENAME = \"speech_commands_v0.02.tar.gz\"\n",
        "GOOGLE_DATASET_URL = \"http://download.tensorflow.org/data/\" + GOOGLE_DATASET_FILENAME\n",
        "GOOGLE_DATASET_DIR = \"google_speech_commands\"\n",
        "CUSTOM_KEYWORDS_FILENAME = \"main.zip\"\n",
        "CUSTOM_KEYWORDS_URL = \"https://github.com/ShawnHymel/custom-speech-commands-dataset/archive/\" + CUSTOM_KEYWORDS_FILENAME\n",
        "CUSTOM_KEYWORDS_DIR = \"custom_keywords\"\n",
        "CUSTOM_KEYWORDS_REPO_NAME = \"custom-speech-commands-dataset-main\"\n",
        "CURATION_SCRIPT = \"dataset-curation.py\"\n",
        "CURATION_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/\" + CURATION_SCRIPT\n",
        "UTILS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/utils.py\"\n",
        "NUM_SAMPLES = 1500    # Target number of samples to mix and send to Edge Impulse\n",
        "WORD_VOL = 1.0        # Relative volume of word in output sample\n",
        "BG_VOL = 0.1          # Relative volume of noise in output sample\n",
        "SAMPLE_TIME = 1.0     # Time (seconds) of output sample\n",
        "SAMPLE_RATE = 16000   # Sample rate (Hz) of output sample\n",
        "BIT_DEPTH = \"PCM_16\"  # Options: [PCM_16, PCM_24, PCM_32, PCM_U8, FLOAT, DOUBLE]\n",
        "BG_DIR = \"_background_noise_\"\n",
        "TEST_RATIO = 0.2      # 20% reserved for test set, rest is for training\n",
        "EI_INGEST_TEST_URL = \"https://ingestion.edgeimpulse.com/api/test/data\"\n",
        "EI_INGEST_TRAIN_URL = \"https://ingestion.edgeimpulse.com/api/training/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVA3SDiQd-jh"
      },
      "source": [
        "### Download Google Speech Commands Dataset\n",
        "!cd {BASE_DIR}\n",
        "!wget {GOOGLE_DATASET_URL}\n",
        "!mkdir {GOOGLE_DATASET_DIR}\n",
        "!echo \"Extracting...\"\n",
        "!tar xfz {GOOGLE_DATASET_FILENAME} -C {GOOGLE_DATASET_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNGNSwX_d_-E"
      },
      "source": [
        "### Pull out background noise directory\n",
        "!cd {BASE_DIR}\n",
        "!mv \"{GOOGLE_DATASET_DIR}/{BG_DIR}\" \"{BG_DIR}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L-vJENeB1S"
      },
      "source": [
        "### (Optional) Download custom dataset--uncomment the code in this cell if you want to use my custom datase\n",
        "\n",
        "## Download, extract, and move dataset to separate directory\n",
        "# !cd {BASE_DIR}\n",
        "# !wget {CUSTOM_KEYWORDS_URL}\n",
        "# !echo \"Extracting...\"\n",
        "# !unzip -q {CUSTOM_KEYWORDS_FILENAME}\n",
        "# !mv \"{CUSTOM_KEYWORDS_REPO_NAME}/{CUSTOM_KEYWORDS_DIR}\" \"{CUSTOM_KEYWORDS_DIR}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nDT4lYEMlY1"
      },
      "source": [
        "### User Settings (do change these)\n",
        "\n",
        "# Location of your custom keyword samples (e.g. \"/content/custom_keywords\")\n",
        "# Leave blank (\"\") for no custom keywords. set to the CUSTOM_KEYWORDS_DIR\n",
        "# variable to use samples from my custom-speech-commands-dataset repo.\n",
        "CUSTOM_DATASET_PATH = \"\"\n",
        "\n",
        "# Edge Impulse > your_project > Dashboard > Keys\n",
        "EI_API_KEY = \"ei_e544...\" \n",
        "\n",
        "# Comma separated words. Must match directory names (that contain samples).\n",
        "# Recommended: use 2 keywords for microcontroller demo\n",
        "TARGETS = \"go, stop\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ve08zgfVLem"
      },
      "source": [
        "### Download curation and utils scripts\n",
        "!wget {CURATION_SCRIPT_URL}\n",
        "!wget {UTILS_SCRIPT_URL}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-M_hViZ0ew"
      },
      "source": [
        "### Perform curation and mixing of samples with background noise\n",
        "!cd {BASE_DIR}\n",
        "!python {CURATION_SCRIPT} \\\n",
        "  -t \"{TARGETS}\" \\\n",
        "  -n {NUM_SAMPLES} \\\n",
        "  -w {WORD_VOL} \\\n",
        "  -g {BG_VOL} \\\n",
        "  -s {SAMPLE_TIME} \\\n",
        "  -r {SAMPLE_RATE} \\\n",
        "  -e {BIT_DEPTH} \\\n",
        "  -b \"{BG_DIR}\" \\\n",
        "  -o \"{OUT_DIR}\" \\\n",
        "  \"{GOOGLE_DATASET_DIR}\" \\\n",
        "  \"{CUSTOM_DATASET_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXd0wH1-hEEX"
      },
      "source": [
        "### Use CLI tool to send curated dataset to Edge Impulse\n",
        "\n",
        "!cd {BASE_DIR}\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Seed with system time\n",
        "random.seed()\n",
        "\n",
        "# Go through each category in our curated dataset\n",
        "for dir in os.listdir(OUT_DIR):\n",
        "  \n",
        "  # Create list of files for one category\n",
        "  paths = []\n",
        "  for filename in os.listdir(os.path.join(OUT_DIR, dir)):\n",
        "    paths.append(os.path.join(OUT_DIR, dir, filename))\n",
        "\n",
        "  # Shuffle and divide into test and training sets\n",
        "  random.shuffle(paths)\n",
        "  num_test_samples = int(TEST_RATIO * len(paths))\n",
        "  test_paths = paths[:num_test_samples]\n",
        "  train_paths = paths[num_test_samples:]\n",
        "\n",
        "  # Create arugments list (as a string) for CLI call\n",
        "  test_paths = ['\"' + s + '\"' for s in test_paths]\n",
        "  test_paths = ' '.join(test_paths)\n",
        "  train_paths = ['\"' + s + '\"' for s in train_paths]\n",
        "  train_paths = ' '.join(train_paths)\n",
        "  \n",
        "  # Send test files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category testing \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {test_paths}\n",
        "\n",
        "  # # Send training files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category training \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {train_paths}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
